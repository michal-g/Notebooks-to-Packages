{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michal-g/Notebooks-to-Packages/blob/main/predicting-ufo-sightings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3MtY9wXUyz-"
      },
      "source": [
        "In this workshop we will take a data analysis pipeline implemented in a Jupyter notebook and convert it to a script that can be run from command-line. We will then convert this script into a Python package: a collection of code modules supporting a pre-defined set of command-line tools.\n",
        "\n",
        "Why do this? This is a very important question. The easiest answer is that, often, there is no reason to. If you are already using Jupyter notebooks, you are familiar with how convenient they make it to create and test an experiment from scratch, allowing you to separate different parts of the experiment across \"cells\" for modular execution. Especially if you want to create plots quickly, notebooks' built-in GUI means that you can write code and produce plots within the same browser window.\n",
        "\n",
        "Jupyter notebooks are great for experiments that are \"linear\" and \"one-off\", meaning that they consist of a single chain of steps carried out one after the other, and that these steps will not have to be updated or rearranged at some point in the future. Indeed, the very visual structure of a notebook reinforces this linear nature: one cell following another, each executed in turn. One can of course choose one's own order of executing individual cells, but this will usually result in errors, and notebooks do not have any built-in mechanism for informing which cell depends on another â€” other than the aforementioned order of the cells themselves.\n",
        "\n",
        "This linearity simplifies things, but it is also extremely limiting in terms of the kinds of experiments we can design. Pipelines which execute heterogenous steps in parallel are off the table, as are pipelines which reuse code from other pipelines without simply copying the text. The modular structure of notebooks is somewhat of an illusion; in reality, the different cells have a very rigid relationship with one another.\n",
        "\n",
        "Jupyter notebooks are also difficult to expand upon beyond the analysis they were designed to carry out. One of the more obvious ways this problem manifests itself is when we try to parametrize an existing experiment. If we are e.g. training a machine learning classifier with a regularization penalty of `alpha=0.01`, and we want to try other values of alpha in a systematic way, there is no way of doing so without manually updating the stated value of `alpha` within the notebook. For testing a handful of values of alpha this is fine, but notebooks quickly become cumbersome if we want to test hundreds of such values. The penalty you pay for being able to execute individual notebook cells within a pretty GUI is the inability to turn cells (or the entire notebook) into functions with arbitrary arguments and argument values.\n",
        "\n",
        "It is difficult to appreciate the full gravity of these considerations until one actually tries to build upon an experiment in Jupyter. Thus we will dispense with any further preamble and introduce a simple data pipeline implemented in a notebook to better understand where exactly the properties inherent to notebooks limit further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr7cKWtRUXVX"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import plotly.express as px\n",
        "\n",
        "!pip install -U kaleido\n",
        "!pip install -U skits\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (14, 9)\n",
        "\n",
        "\n",
        "base_url = 'https://nuforc.org/webreports'\n",
        "grab = requests.get('/'.join([base_url, 'ndxevent.html']))\n",
        "soup = BeautifulSoup(grab.text, 'html.parser')\n",
        "\n",
        "sightings = []\n",
        "col_labels = ['Date', 'City', 'Region', 'Country', 'Shape', 'Duration', \n",
        "              'Summary', 'Posted', 'Images']\n",
        "\n",
        "for link in soup('a', string=re.compile(\"[0-9]{2}\\/2000\")):\n",
        "  data = link.get('href')\n",
        "  grab_date = requests.get('/'.join([base_url, data]))\n",
        "  date_soup = BeautifulSoup(grab_date.text, 'html.parser')\n",
        "\n",
        "  for row in date_soup('tr'):\n",
        "    cols = row.find_all('td')\n",
        "\n",
        "    if cols:\n",
        "      cur_sighting = None\n",
        "\n",
        "      for lbl, col in zip(itertools.cycle(col_labels), cols):\n",
        "        if lbl == 'Date':\n",
        "          if cur_sighting is not None:\n",
        "            sightings.append(cur_sighting)\n",
        "\n",
        "          cur_sighting = {'Date': col.string}\n",
        "\n",
        "        else:\n",
        "          cur_sighting[lbl] = col.string\n",
        "\n",
        "      if cur_sighting is not None:\n",
        "        sightings.append(cur_sighting)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOJjSMzC5uf4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "valid_states = {\n",
        "    'AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI',\n",
        "    'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN',\n",
        "    'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH',\n",
        "    'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX',\n",
        "    'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY'\n",
        "    }\n",
        "\n",
        "sights_df = pd.DataFrame(sightings)\n",
        "sights_df = sights_df.loc[(sights_df.Country == 'USA') & sights_df.Region.isin(valid_states), :]\n",
        "sights_df['Date'] = pd.to_datetime([dt.split()[0] for dt in sights_df['Date']], format='%m/%d/%y')\n",
        "\n",
        "print(sights_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la99laOpqoH0"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "counts = sights_df.groupby('Region').size()\n",
        "\n",
        "fig = px.choropleth(locations=[str(x) for x in counts.index],\n",
        "                    locationmode=\"USA-states\",\n",
        "                    color=counts.values, range_color=[0, counts.max()],\n",
        "                    scope=\"usa\",\n",
        "                    color_continuous_scale=['white', 'black'])\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc3GctFyfgRu"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "counts = sights_df.groupby(['Date', 'Region']).size()\n",
        "plt_files = list()\n",
        "\n",
        "for dt, dt_counts in counts.groupby('Date'):\n",
        "    date_lbl = dt.strftime('%F')\n",
        "\n",
        "    fig = px.choropleth(locations=[str(x) for x in dt_counts.index.get_level_values('Region')],\n",
        "                        locationmode=\"USA-states\", title=date_lbl,\n",
        "                        color=dt_counts.values, range_color=[0, 100],\n",
        "                        scope=\"usa\", color_continuous_scale=['white', 'black'])\n",
        "\n",
        "    plt_file = f\"counts_{date_lbl}.png\"\n",
        "    fig.write_image(plt_file, format='png')\n",
        "    plt_files += [imageio.imread(plt_file)]\n",
        "\n",
        "imageio.mimsave(\"counts.gif\", plt_files, duration=0.03)\n",
        "from IPython.display import Image\n",
        "Image(filename=\"counts.gif\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ-Wd3RdENOZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from skits.preprocessing import (ReversibleImputer,\n",
        "                                 DifferenceTransformer)\n",
        "from skits.pipeline import ForecasterPipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from skits.pipeline import ForecasterPipeline\n",
        "from skits.feature_extraction import (AutoregressiveTransformer,\n",
        "                                      SeasonalTransformer)\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
        "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
        "                              RandomForestRegressor)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "pipeline = ForecasterPipeline([\n",
        "    ('pre_scaler', StandardScaler()),\n",
        "    ('features', FeatureUnion([\n",
        "        ('ar_features', AutoregressiveTransformer(num_lags=3)),\n",
        "        ('seasonal_features', SeasonalTransformer(seasonal_period=10)),\n",
        "    ])),\n",
        "    ('post_feature_imputer', ReversibleImputer()),\n",
        "    ('post_feature_scaler', StandardScaler()),\n",
        "    ('regressor', LinearRegression(fit_intercept=True))\n",
        "    ])\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "ca_counts = counts.loc[(slice(None), 'CA')]\n",
        "ca_dates = ca_counts.index.get_level_values('Date').values.reshape(-1, 1)\n",
        "ca_values = ca_counts.values\n",
        "\n",
        "real_values = list()\n",
        "pred_values = list()\n",
        "\n",
        "for train_index, test_index in tscv.split(ca_counts):\n",
        "    pipeline.fit(ca_dates[train_index], ca_values[train_index])\n",
        "\n",
        "    preds = pipeline.predict(ca_dates[test_index], to_scale=True)\n",
        "\n",
        "    real_values += ca_values[test_index].flatten().tolist()\n",
        "    pred_values += preds.flatten().tolist()\n",
        "\n",
        "    plt.plot(ca_dates[test_index], ca_values[test_index], color='black')\n",
        "    plt.plot(ca_dates[test_index], preds, color='red')\n",
        "\n",
        "print(f\"MSE: {format(((np.array(real_values) - np.array(pred_values)) ** 2).sum(), '.1f')}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO4h0G0XsMerq650XjopBXm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}