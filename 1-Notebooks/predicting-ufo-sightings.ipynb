{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/michal-g/Notebooks-to-Packages/blob/main/1-Notebooks/predicting-ufo-sightings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by acquiring the data on UFO sightings. Fortunately, github user Link Wentz has been nice enough to download\n",
    "it from the website and parse it into a reasonable format, which we make available here.\n",
    "\n",
    "We notice however that there are irregularities in some of the entries, which we would like to clean up!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOJjSMzC5uf4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sightings = pd.read_csv('../nuforc_events_complete.csv',\n",
    "                        usecols=['event_time', 'city', 'state',\n",
    "                                 'shape', 'duration', 'summary'])\n",
    "\n",
    "print(sightings.head())\n",
    "print('----------')\n",
    "print(\"Number of duplicate rows:\")\n",
    "print(sightings.duplicated().sum())\n",
    "print('----------')\n",
    "print(\"# of rows by state:\")\n",
    "print(sightings.state.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# we will be very careful about filtering sightings that can be mapped to states\n",
    "valid_states = {\n",
    "    'AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI',\n",
    "    'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN',\n",
    "    'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH',\n",
    "    'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX',\n",
    "    'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY'\n",
    "    }\n",
    "sightings = sightings.loc[sightings.state.isin(valid_states), :]\n",
    "\n",
    "# parse the date information into a more useful format\n",
    "sightings['event_time'] = pd.to_datetime(sightings.event_time,\n",
    "                                         format=\"%Y-%m-%dT%H:%M:%SZ\", errors='coerce')\n",
    "print(f\"{sightings['event_time'].isna().sum()} rows with missing times\")\n",
    "\n",
    "sightings = sightings.loc[~sightings['event_time'].isna(), :]\n",
    "print(sightings.head())\n",
    "print(sightings.tail())\n",
    "print(\"---------\")\n",
    "print(f\"{sightings.shape[0]} total sightings\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's examine the total number of sightings from across the year for each state. We see that California\n",
    "is a very popular destination!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "la99laOpqoH0"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "state_totals = sightings.groupby('state').size()\n",
    "\n",
    "fig = px.choropleth(locations=[str(x) for x in state_totals.index],\n",
    "                    scope=\"usa\", locationmode=\"USA-states\",\n",
    "                    color=state_totals.values,\n",
    "                    range_color=[0, state_totals.max()],\n",
    "                    color_continuous_scale=['white', 'red'])\n",
    "\n",
    "fig.update_layout(coloraxis_colorbar=dict(title=\"Sightings\"))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To better understand how sightings vary across the year, we'll animate the weekly numbers of sightings for each state.\n",
    "Note the use of multi-dimensional `groupby` to create a table with sightings broken down by time and place."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vc3GctFyfgRu"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# create a Year x State table containing total yearly sightings for each state;\n",
    "# note that we have to take into account \"missing\" years that did not have any\n",
    "# sightings in any states\n",
    "state_yearlies = sightings.groupby(\n",
    "    [sightings.event_time.dt.year, 'state']).size().unstack().fillna(0)\n",
    "\n",
    "# create a list of individual files and a place to save them\n",
    "plt_files = list()\n",
    "!mkdir -p map-plots\n",
    "\n",
    "# create a map of sightings by state for each week\n",
    "for year, year_counts in state_yearlies.iterrows():\n",
    "    state_locs = [str(x) for x in year_counts.index.get_level_values('state')]\n",
    "\n",
    "    fig = px.choropleth(locations=state_locs, locationmode=\"USA-states\",\n",
    "                        title=year, scope='usa',\n",
    "                        color=year_counts.values, range_color=[0, 10],\n",
    "                        color_continuous_scale=['white', 'black'])\n",
    "    fig.update_layout(coloraxis_colorbar=dict(title=\"Sightings\"))\n",
    "\n",
    "    # save the map to file and keep track of the file name\n",
    "    plt_file = Path(\"map-plots\", f\"counts_{year}.png\")\n",
    "    fig.write_image(plt_file, format='png')\n",
    "    plt_files += [imageio.v2.imread(plt_file)]\n",
    "\n",
    "# create an animation using the individual weekly maps\n",
    "imageio.mimsave(Path(\"map-plots\", \"counts.gif\"), plt_files, duration=0.03)\n",
    "Image(filename=str(Path(\"map-plots\", \"counts.gif\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "A time series regression model can predict the number of sightings that will take place on each of a series of weeks.\n",
    "We set up a cross-validation process in which the model makes predictions across chunks of our year range by training\n",
    "on preceding chunks. The quality of the predictions is judged using root-mean-squared-error, which is useful as it\n",
    "places the error in the same scale as the original data.\n",
    "\n",
    "To improve our model's performance, we can try to swap out or rearrange elements of the prediction pipeline, as well\n",
    "as tune the hyper-parameters of the input transformers and prediction algorithms. This is left as an exercise\n",
    "to the reader!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 9)\n",
    "import numpy as np\n",
    "\n",
    "from skits.preprocessing import ReversibleImputer\n",
    "from skits.pipeline import ForecasterPipeline\n",
    "from skits.feature_extraction import (AutoregressiveTransformer,\n",
    "                                      SeasonalTransformer)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "pipeline = ForecasterPipeline([\n",
    "    ('pre_scaler', StandardScaler()),\n",
    "    ('features', FeatureUnion([\n",
    "      ('ar_features', AutoregressiveTransformer(num_lags=52)),\n",
    "      ('seasonal_features', SeasonalTransformer(seasonal_period=52)),\n",
    "      ])),\n",
    "    ('post_feature_imputer', ReversibleImputer()),\n",
    "    ('post_feature_scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "    ])\n",
    "\n",
    "# assets and specially formatted objects used by the prediction pipeline\n",
    "# scikit-learn wants Xs to be 2-dimensional and ys to be 1-dimensional\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "cali_yearlies = state_yearlies.CA.reindex(\n",
    "    range(state_yearlies.index[0], state_yearlies.index[-1] + 1),\n",
    "    fill_value=0.\n",
    "    )\n",
    "\n",
    "print(cali_yearlies.tail())\n",
    "\n",
    "cali_dates = cali_yearlies.index.values.reshape(-1, 1)\n",
    "cali_values = cali_yearlies.values\n",
    "\n",
    "real_values = list()\n",
    "pred_values = list()\n",
    "\n",
    "# for each cross-validation fold, use the training samples in the fold to\n",
    "# train the pipeline and the remaining samples to test it\n",
    "for train_index, test_index in tscv.split(cali_yearlies):\n",
    "    pipeline.fit(cali_dates[train_index], cali_values[train_index])\n",
    "    preds = pipeline.predict(cali_dates[test_index], to_scale=True)\n",
    "\n",
    "    # we'll keep track of the actual sightings and the predicted sightings from\n",
    "    # each c-v fold for future reference\n",
    "    real_values += cali_values[test_index].flatten().tolist()\n",
    "    pred_values += preds.flatten().tolist()\n",
    "\n",
    "    plt.plot(cali_dates[test_index], cali_values[test_index], color='black')\n",
    "    plt.plot(cali_dates[test_index], preds, color='red')\n",
    "\n",
    "# measure the quality of the predictions using root-mean-squared-error\n",
    "rmse_val = ((np.array(real_values) - np.array(pred_values)) ** 2).mean() ** 0.5\n",
    "print(f\"RMSE: {format(rmse_val, '.3f')}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
