{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/michal-g/Notebooks-to-Packages/blob/main/predicting-ufo-sightings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by scraping the datasets from the sightings reports website. The reports portal contains a link to a table for each historical month, so we have to go through the links and parse each table individually. For now, we will only consider sightings from the 1990s to speed up this step.\n",
    "\n",
    "Note that at this stage it is easier to store the sightings as a list of dictionaries that can be progressively grown as we scan through the website for sighting records."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sr7cKWtRUXVX"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# initialize assets for scraping the reports portal\n",
    "base_url = 'https://nuforc.org/webreports'\n",
    "grab = requests.get('/'.join([base_url, 'ndxevent.html']))\n",
    "\n",
    "# initialize data structures for storing parsed data\n",
    "sightings = []\n",
    "col_labels = ['Date', 'City', 'State', 'Country', 'Shape', 'Duration',\n",
    "              'Summary', 'Posted', 'Images']\n",
    "\n",
    "# for each link to a month's data, create assets for scraping that table\n",
    "for month_link in BeautifulSoup(grab.text, 'html.parser')(\n",
    "    'a', string=re.compile(\"[0-9]{2}\\/199[0-9]\")):\n",
    "  month_grab = requests.get('/'.join([base_url, month_link.get('href')]))\n",
    "\n",
    "  # the HTML formatting is kind of weird; we first grab the outermost of a\n",
    "  # recursively defined set of table elements\n",
    "  table_data = BeautifulSoup(month_grab.text, 'html.parser')('tr')[1]('td')\n",
    "  cur_sighting = None\n",
    "\n",
    "  # then we loop over a set of table entries that are defined as one big row???\n",
    "  # maybe there's an easier way to do this but that's ok\n",
    "  for lbl, col in zip(itertools.cycle(col_labels), table_data):\n",
    "    if lbl == 'Date':\n",
    "      if cur_sighting is not None:\n",
    "        sightings.append(cur_sighting)\n",
    "\n",
    "      # start a new sighting record, after adding the last record to the list of\n",
    "      # sightings if this is not the first row\n",
    "      cur_sighting = {'Date': col.string}\n",
    "\n",
    "    else:\n",
    "      cur_sighting[lbl] = col.string\n",
    "\n",
    "  # accounting for the last row\n",
    "  if cur_sighting is not None:\n",
    "    sightings.append(cur_sighting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we've created a list containing all the sightings, we can convert it into a tabular format that is more convenient for us. We will remove any sightings from outside the US to simplify further analyses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOJjSMzC5uf4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# we will be very careful about filtering sightings that can be mapped to states\n",
    "valid_states = {\n",
    "    'AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI',\n",
    "    'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN',\n",
    "    'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH',\n",
    "    'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX',\n",
    "    'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY'\n",
    "    }\n",
    "\n",
    "# create a table for the sightings data and only consider valid unique sightings\n",
    "sights_df = pd.DataFrame(sightings).drop_duplicates()\n",
    "sights_df = sights_df.loc[(sights_df.Country == 'USA')\n",
    "                          & sights_df.State.isin(valid_states), :]\n",
    "\n",
    "# parse the date information into a more useful format\n",
    "sights_df['Date'] = pd.to_datetime([dt.split()[0] for dt in sights_df['Date']],\n",
    "                                   format='%m/%d/%y')\n",
    "\n",
    "print(sights_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's examine the total number of sightings from across the year for each state. We see that California is a very popular destination!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "la99laOpqoH0"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "state_totals = sights_df.groupby('State').size()\n",
    "\n",
    "fig = px.choropleth(locations=[str(x) for x in state_totals.index],\n",
    "                    scope=\"usa\", locationmode=\"USA-states\",\n",
    "                    color=state_totals.values,\n",
    "                    range_color=[0, state_totals.max()],\n",
    "                    color_continuous_scale=['white', 'red'])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To better understand how sightings vary across the year, we'll animate the weekly numbers of sightings for each state."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vc3GctFyfgRu"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# create a Week x State table containing total weekly sightings for each state;\n",
    "# note that we have to take into account \"missing\" weeks that did not have any\n",
    "# sightings in any states\n",
    "state_table = sights_df.groupby(['Date', 'State']).size().unstack().fillna(0)\n",
    "state_table = state_table.reindex(\n",
    "  index=pd.date_range('01-01-1990', '12-31-1999'), fill_value=0).sort_index()\n",
    "state_weeklies = state_table.groupby(\n",
    "  pd.Grouper(axis=0, freq='W', sort=True)).sum()\n",
    "\n",
    "# create a list of individual files and a place to save them\n",
    "plt_files = list()\n",
    "!mkdir -p map-plots\n",
    "\n",
    "# create a map of sightings by state for each week\n",
    "for week, week_counts in state_weeklies.iterrows():\n",
    "    day_lbl = week.strftime('%F')\n",
    "    state_locs = [str(x) for x in week_counts.index.get_level_values('State')]\n",
    "\n",
    "    fig = px.choropleth(locations=state_locs, locationmode=\"USA-states\",\n",
    "                        title=day_lbl, scope='usa',\n",
    "                        color=week_counts.values, range_color=[0, 10],\n",
    "                        color_continuous_scale=['white', 'black'])\n",
    "\n",
    "    # save the map to file and keep track of the file name\n",
    "    plt_file = Path(\"map-plots\", f\"counts_{day_lbl}.png\")\n",
    "    fig.write_image(plt_file, format='png')\n",
    "    plt_files += [imageio.v2.imread(plt_file)]\n",
    "\n",
    "# create an animation using the individual weekly maps\n",
    "imageio.mimsave(Path(\"map-plots\", \"counts.gif\"), plt_files, duration=0.03)\n",
    "Image(filename=str(Path(\"map-plots\", \"counts.gif\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "A time series regression model can predict the number of sightings that will take place on each of a series of weeks. We set up a cross-validation process in which the model makes predictions across chunks of our year range by training on preceding chunks. The quality of the predictions is judged using root-mean-squared-error, which is useful as it places the error in the same scale as the original data.\n",
    "\n",
    "To improve our model's performance, we can try to swap out or rearrange elements of the prediction pipeline, as well as tune the hyper-parameters of the input transformers and prediction algorithms. This is left as an exercise to the reader!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJ-Wd3RdENOZ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 9)\n",
    "import numpy as np\n",
    "\n",
    "from skits.preprocessing import ReversibleImputer\n",
    "from skits.pipeline import ForecasterPipeline\n",
    "from skits.feature_extraction import (AutoregressiveTransformer,\n",
    "                                      SeasonalTransformer)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "pipeline = ForecasterPipeline([\n",
    "    ('pre_scaler', StandardScaler()),\n",
    "    ('features', FeatureUnion([\n",
    "      ('ar_features', AutoregressiveTransformer(num_lags=52)),\n",
    "      ('seasonal_features', SeasonalTransformer(seasonal_period=52)),\n",
    "      ])),\n",
    "    ('post_feature_imputer', ReversibleImputer()),\n",
    "    ('post_feature_scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "    ])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "cali_weeklies = state_weeklies.CA\n",
    "cali_dates = cali_weeklies.index.values.reshape(-1, 1)\n",
    "cali_values = cali_weeklies.values\n",
    "\n",
    "real_values = list()\n",
    "pred_values = list()\n",
    "\n",
    "for train_index, test_index in tscv.split(cali_weeklies):\n",
    "    pipeline.fit(cali_dates[train_index], cali_values[train_index])\n",
    "    preds = pipeline.predict(cali_dates[test_index], to_scale=True)\n",
    "\n",
    "    real_values += cali_values[test_index].flatten().tolist()\n",
    "    pred_values += preds.flatten().tolist()\n",
    "\n",
    "    plt.plot(cali_dates[test_index], cali_values[test_index], color='black')\n",
    "    plt.plot(cali_dates[test_index], preds, color='red')\n",
    "\n",
    "rmse_val = ((np.array(real_values) - np.array(pred_values)) ** 2).mean() ** 0.5\n",
    "print(f\"RMSE: {format(rmse_val, '.3f')}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
